{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM0a9f8ghklZ+NAr1LXifTW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aftabgazali/Combining-Different-Models-for-Ensemble-Learning.ipynb/blob/main/Combining_Different_Models_for_Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "w6-0maG7F24l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63bEb9II-sOy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris_data = sns.load_dataset('iris')\n",
        "\n",
        "iris_data.head()"
      ],
      "metadata": {
        "id": "o0qMLMCF-2ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing & Model Building"
      ],
      "metadata": {
        "id": "2bnxFuHmC7zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = iris_data.iloc[:100, [1,2]].values, iris_data.iloc[:100, -1]"
      ],
      "metadata": {
        "id": "J61Iz0U4-8VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "print(f\"Class labels {np.unique(y)}\")"
      ],
      "metadata": {
        "id": "8zM--vaJ_HZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
      ],
      "metadata": {
        "id": "2XafQl6B_jB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "mCziMfv8CVFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Simple Models\n",
        "\n",
        "***Note: While ROC AUC is a valuable metric for evaluating binary classifiers, it may not provide a complete picture in multi-class classification scenarios. In such cases, you might consider techniques like micro-average ROC AUC or macro-average ROC AUC to aggregate performance across multiple classes. Check `get_scorer_names` to get list of scores which you can use***"
      ],
      "metadata": {
        "id": "f-cMZu96_uba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import get_scorer_names\n",
        "get_scorer_names()"
      ],
      "metadata": {
        "id": "_dz_eQE8SWpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model_v0 = LogisticRegression(penalty='l2', C=0.001,solver='lbfgs',multi_class='ovr')\n",
        "model_v1 = DecisionTreeClassifier(max_depth=1, criterion='entropy')\n",
        "model_v2 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski', )\n",
        "\n",
        "\n",
        "pipeline_1 = make_pipeline(StandardScaler(),model_v0)\n",
        "pipeline_2 = make_pipeline(StandardScaler(), model_v2)\n",
        "\n",
        "model_labels = ['Logistic Regression', 'Decision Tree', 'KNN']\n",
        "\n",
        "for model, label in zip([pipeline_1,model_v1,pipeline_2], model_labels):\n",
        "  scores = cross_val_score(estimator=model, X = X_train, y = y_train, cv =10, scoring='roc_auc')\n",
        "  print(f\"ROC AUC: {scores.mean():.2f} | for Model: {label}\")\n"
      ],
      "metadata": {
        "id": "4ptVjfTV_t8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Custom Ensemble Model\n",
        "\n",
        "***Note:*** *A `'hard'` voting indicates it's based on majority vote, `'soft'` voting indicates it's based on the mean*"
      ],
      "metadata": {
        "id": "_QoVN1H0EqS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "ensemble_classifier = VotingClassifier(estimators=[\n",
        "    ('pipeline-1', pipeline_1),\n",
        "    ('pipeline-2', model_v1),\n",
        "    ('pipeline-3', pipeline_2)\n",
        "], voting='soft')\n",
        "\n",
        "\n",
        "model_labels.append('Ensemble')\n",
        "\n",
        "for model, label in zip([pipeline_1,model_v1,pipeline_2, ensemble_classifier], model_labels):\n",
        "  scores = cross_val_score(estimator=model, X = X_train, y = y_train, cv =10, scoring='accuracy')\n",
        "  print(f\"ROC AUC: {scores.mean():.2f} | for Model: {label}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AjbzWyAJEt91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating and tuning the ensemble classifier\n",
        "\n",
        "*Plotting out ROC AUC curves is essential to understand how model performed for TP vs FP*"
      ],
      "metadata": {
        "id": "1Mvig60BHUct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.base import clone\n",
        "colors = ['green','yellow','red','blue']\n",
        "\n",
        "line_styles=[':','--','-.','-']\n",
        "\n",
        "# put all classifiers into one list\n",
        "all_classifiers = [pipeline_1,model_v1,pipeline_2, ensemble_classifier]\n",
        "for model,label,color, line_style in zip(all_classifiers,model_labels, colors, line_styles):\n",
        "  cloned_model = clone(model)\n",
        "  y_pred = cloned_model.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
        "  fpr, tpr, thresholds = roc_curve(y_true=y_test,y_score=y_pred)\n",
        "  roc_auc = auc(x=fpr, y=tpr)\n",
        "  plt.plot(fpr, tpr, color=color,linestyle=line_style, label=f\"{label} (auc = {roc_auc:.2f})\")\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0, 1], [0, 1],linestyle='--',color='gray',linewidth=2)\n",
        "plt.xlim([-0.1, 1.1])\n",
        "plt.ylim([-0.1, 1.1])\n",
        "plt.grid(alpha=0.5)\n",
        "plt.xlabel('False positive rate (FPR)')\n",
        "plt.ylabel('True positive rate (TPR)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ybkL3LRlHZ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper-parameter tunning"
      ],
      "metadata": {
        "id": "WC5IHlElQ41c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_classifier.get_params()"
      ],
      "metadata": {
        "id": "o-9uK64pM5vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'pipeline-2__max_depth':[1,2,3,4,5,6,7,8],\n",
        "    'pipeline-1__logisticregression__C': [0.0001, 00.1,1, 100.0]}\n",
        "grid = GridSearchCV(estimator=ensemble_classifier,param_grid=params,cv=10,scoring='roc_auc')\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "M0hm1P3nLgT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best params: {grid.best_params_}\")"
      ],
      "metadata": {
        "id": "jzNCjV2SOn2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best score: {grid.best_score_*100:.2f}%\")"
      ],
      "metadata": {
        "id": "KzJDTNOwRcha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging in Action"
      ],
      "metadata": {
        "id": "_Lv-RkP2TbLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Wine dataset"
      ],
      "metadata": {
        "id": "sXNROt0OTnXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine_data = load_wine()\n",
        "\n",
        "df = pd.DataFrame(data = wine_data.data, columns = wine_data.feature_names)\n",
        "df['target'] = wine_data.target\n",
        "df.head()"
      ],
      "metadata": {
        "id": "A7xN5TGkTmkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Class labels: {np.unique(df['target'])}\")"
      ],
      "metadata": {
        "id": "RjwWmjVyTdIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Let's drop one class as ROC AUC will not work for multi-classification problem*"
      ],
      "metadata": {
        "id": "-_2cn2tfUEQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df[df['target'] != 2]\n",
        "new_df.head()"
      ],
      "metadata": {
        "id": "wndkNuazUKJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Class labels: {np.unique(new_df['target'])}\")"
      ],
      "metadata": {
        "id": "gIckF9sIUT_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = new_df.iloc[:,:-1].values, new_df.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "SwkgIT2NYq1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Split the data into 80/20 %*"
      ],
      "metadata": {
        "id": "GjmjjhJfUaY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
      ],
      "metadata": {
        "id": "l69UT4q2Ud1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "ZN7SgLmHYQPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.bincount(y_train)"
      ],
      "metadata": {
        "id": "Psm0r9x1UsUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We will use an unpruned decision tree as the base classifier and create an\n",
        "ensemble of 500 decision trees fit on different bootstrap samples of the training dataset*"
      ],
      "metadata": {
        "id": "-H-SdRz7XGs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(criterion='entropy')\n",
        "\n",
        "bag_model = BaggingClassifier(estimator=tree, n_estimators=500,bootstrap=True,bootstrap_features=False)"
      ],
      "metadata": {
        "id": "YjYJopAuXGPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "tree = tree.fit(X_train, y_train)\n",
        "y_pred_tree = tree.predict(X_test)\n",
        "\n",
        "bag_model = bag_model.fit(X_train, y_train)\n",
        "y_pred_bag = bag_model.predict(X_test)\n",
        "\n",
        "print(f\"Tree Model Accuracy: {accuracy_score(y_test, y_pred_tree)*100:.2f}\")\n",
        "print(f\"Bagging Model Accuracy: {accuracy_score(y_test, y_pred_bag)*100:.2f}\")"
      ],
      "metadata": {
        "id": "7RPaz7sNXnKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADA Boosting in Action\n",
        "**AdaBoost trains decision tree stumps based on errors of the previous decision tree stump. In particular, the errors are used to compute sample weights in each round as well as for computing a clas-\n",
        "sifier weight for each decision tree stump when combining the individual stumps into an ensemble. Westop training once a maximum number of iterations (decision tree stumps) is reached.**"
      ],
      "metadata": {
        "id": "cGv0aQUaZVdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# Create a prunned DT as a weak learner for ADA Boost\n",
        "tree = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
        "ada = AdaBoostClassifier(estimator=tree, n_estimators=500,learning_rate=0.01)\n",
        "\n",
        "y_pred_tree = tree.fit(X_train, y_train).predict(X_test)\n",
        "y_pred_train_tree = tree.predict(X_train)\n",
        "y_pred_boost = ada.fit(X_train,y_train).predict(X_test)\n",
        "y_pred_train_boost = ada.predict(X_train)"
      ],
      "metadata": {
        "id": "aiv_NisSZYjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*You can see DT stump, underfits unlike the unprunned DT in Bagging*\n",
        "\n",
        "**ADA boost works well, with the training examples however, you can also see that we introduced additional variance with our attempt to reduce the model\n",
        "biasâ€”a greater gap between training and test performance**"
      ],
      "metadata": {
        "id": "PrAypLnhapr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tree Stump Training Accuracy: {accuracy_score(y_train, y_pred_train_tree)}  | Testing Accuracy: {accuracy_score(y_test, y_pred_tree)*100:.2f}\")\n",
        "print(f\"Ada Boost Training Accuracy: {accuracy_score(y_train, y_pred_train_boost)*100:.2f} | Testing Accuracy: {accuracy_score(y_test, y_pred_boost)*100:.2f}\")"
      ],
      "metadata": {
        "id": "3XYtCDFsaTU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boost in Action\n",
        "\n",
        "**Like AdaBoost,\n",
        "gradient boosting fits decision trees in an iterative fashion using prediction errors. However, gradient\n",
        "boosting trees are usually deeper than decision tree stumps and have typically a maximum depth of\n",
        "3 to 6 (or a maximum number of 8 to 64 leaf nodes). Also, in contrast to AdaBoost, gradient boosting\n",
        "does not use the prediction errors for assigning sample weights; they are used directly to form the\n",
        "target variable for fitting the next tree. Moreover, instead of having an individual weighting term for\n",
        "each tree, like in AdaBoost, gradient boosting uses a global learning rate that is the same for each tree.**"
      ],
      "metadata": {
        "id": "XHEpS-Ttcz1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgboost = xgb.XGBClassifier(n_estimators=200, learning_rate=0.1,max_depth=4)\n",
        "\n",
        "xgboost = xgboost.fit(X_train, y_train)\n",
        "y_train_pred = xgboost.predict(X_train)\n",
        "y_test_pred = xgboost.predict(X_test)\n",
        "\n",
        "print(f\"XGBoost Training Accuracy: {accuracy_score(y_train, y_train_pred)}  | Testing Accuracy: {accuracy_score(y_test, y_test_pred)*100:.2f}\")"
      ],
      "metadata": {
        "id": "RoKlQFbDc3HF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}